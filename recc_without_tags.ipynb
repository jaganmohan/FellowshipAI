{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import datetime\n",
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil,sqrt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"C:/Users/Jazzy/Academia/ML/lastfm/user_taggedartists-timestamps.dat\"\n",
    "train_file = \"train.txt\"\n",
    "valid_file = \"valid.txt\"\n",
    "\n",
    "ENTROPY_CUTOFF = 3.0\n",
    "\n",
    "MIN_OCCURRENCES = 10\n",
    "MIN_VALID_SEQ_LEN = 4\n",
    "#MAX_VALID_SEQ_LEN = 500\n",
    "\n",
    "def processdata():\n",
    "    data = list()\n",
    "    i_occ = collections.defaultdict(lambda: 0)\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            uid, aid, tid, ts = line.strip().split('\\t')\n",
    "            data.append((uid,aid,tid,ts))\n",
    "            i_occ[aid] += 1\n",
    "        \n",
    "    tmp_data = collections.defaultdict(list)\n",
    "    for uid,aid,tid,ts in data:\n",
    "        if i_occ[aid] > MIN_OCCURRENCES:\n",
    "            tmp_data[uid].append((ts,aid,tid))\n",
    "            \n",
    "    seq_data = dict()\n",
    "    for uid in tmp_data:\n",
    "        seq = [(aid,tid) for ts,aid,tid in sorted(tmp_data[uid])]\n",
    "        seq_data[uid] = seq\n",
    "        \n",
    "    train = dict()\n",
    "    valid = dict()\n",
    "    for user,seq in seq_data.items():\n",
    "        if len(seq) > MIN_OCCURRENCES:\n",
    "            cutoff = max(MIN_VALID_SEQ_LEN, int(round(0.25 * len(seq))))\n",
    "            train[user] = seq[:-cutoff]\n",
    "            valid[user] = seq[-cutoff:]\n",
    "          \n",
    "    items = list()\n",
    "    tags = list()\n",
    "    for x,y in itertools.chain(*train.values()):\n",
    "        items.append(x)\n",
    "        tags.append(y)\n",
    "    items = set(items)\n",
    "    tags = set(tags)\n",
    "    users = set(train.keys())\n",
    "    user2id = dict(zip(users, range(1,len(users)+1)))\n",
    "    items2id = dict(zip(items, range(1,len(items)+1)))\n",
    "    tags2id = dict(zip(tags, range(1,len(tags)+1)))\n",
    "    train_data = dict()\n",
    "    valid_data = dict()\n",
    "    \n",
    "    for user in users:\n",
    "        train_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]), train[user]))\n",
    "        valid_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]),\n",
    "                                             filter(lambda x: x[0] in items and x[1] in tags, valid[user])))\n",
    "        \n",
    "    with open(train_file, \"w\") as t, open(valid_file, \"w\") as v:\n",
    "        for uid in user2id.values():\n",
    "            for aid,tid in train_data[uid]:\n",
    "                t.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "            for aid,tid in valid_data[uid]:\n",
    "                v.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "                \n",
    "\n",
    "processdata()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, seq_dict):\n",
    "        self._seq_dict = seq_dict\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "        # These variables are set after calling `prepare_batches`.\n",
    "        self._users_in_batches = None\n",
    "        self._batches = None\n",
    "        self._seq_lengths = None\n",
    "        self._chunk_size = None\n",
    "\n",
    "    @property\n",
    "    def num_users(self):\n",
    "        return self._num_users\n",
    "\n",
    "    @property\n",
    "    def num_items(self):\n",
    "        return self._num_items\n",
    "    \n",
    "    @property\n",
    "    def num_tags(self):\n",
    "        return self._num_tags\n",
    "\n",
    "    @property\n",
    "    def num_triplets(self):\n",
    "        return sum(len(seq) for u, seq in self.sequences())\n",
    "\n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return len(self._batches)\n",
    "\n",
    "    @property\n",
    "    def users_in_batches(self):\n",
    "        if self._users_in_batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return self._users_in_batches\n",
    "    \n",
    "    def __getitem__(self, u):\n",
    "        return self._seq_dict[u]\n",
    "\n",
    "    def sequences(self):\n",
    "        return self._seq_dict.items()\n",
    "\n",
    "    def iter_batches(self, order=None):\n",
    "        if order is None:\n",
    "            order = range(self.num_batches)\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        cs = self._chunk_size\n",
    "        def iter_batch(batch, seq_length):\n",
    "            num_cols = batch.shape[1]\n",
    "            for i, z in enumerate(range(0, num_cols - 1, cs)):\n",
    "                inputs = batch[:,z:z+cs,:]\n",
    "                targets = batch[:,(z+1):(z+cs+1),1]\n",
    "                yield (inputs, targets, seq_length[:,i])\n",
    "        for i in order:\n",
    "            yield iter_batch(self._batches[i], self._seq_lengths[i])\n",
    "\n",
    "    def prepare_batches(self, chunk_size, batch_size, batches_like=None):\n",
    "        # Spread users over batches.\n",
    "        if batches_like is not None:\n",
    "            self._users_in_batches = batches_like.users_in_batches\n",
    "        else:\n",
    "            self._users_in_batches = Dataset._assign_users_to_batches(\n",
    "                    batch_size, self._seq_dict)\n",
    "        # Build the batches and record the corresponding valid sequence lengths.\n",
    "        self._chunk_size = chunk_size\n",
    "        self._batches = list()\n",
    "        self._seq_lengths = list()\n",
    "        for users in self._users_in_batches:\n",
    "#           print(\"users :\",users)\n",
    "            lengths = tuple(len(self[u]) for u in users)\n",
    "#           print(\"lengths: \",lengths)\n",
    "            num_chunks = int(ceil(max(max(lengths) - 1, chunk_size)\n",
    "                    / chunk_size))\n",
    "            num_cols = num_chunks * chunk_size + 1\n",
    "#           print(\"number of columns: \",num_cols)\n",
    "            batch = np.zeros((batch_size, num_cols, 3), dtype=np.int32)\n",
    "            seq_length = np.zeros((batch_size, num_chunks), dtype=np.int32)\n",
    "            for i, (user, length) in enumerate(zip(users, lengths)):\n",
    "                # Assign the values to the batch.\n",
    "                batch[i,:length,0] = user\n",
    "                batch[i,:length,1:] = self[user]\n",
    "#               print(\"self[user]\",user,self[user])\n",
    "                # Compute and assign the valid sequence lengths.\n",
    "                q, r = divmod(max(0, min(num_cols, length) - 1), chunk_size)\n",
    "                seq_length[i,:q] = chunk_size\n",
    "                if r > 0:\n",
    "                    seq_length[i,q] = r\n",
    "#           print(\"batch: \", batch)\n",
    "            self._batches.append(batch)\n",
    "            self._seq_lengths.append(seq_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def _assign_users_to_batches(batch_size, seq_dict):\n",
    "        lengths, users = zip(*sorted(((len(seq), u)\n",
    "                for u, seq in seq_dict.items()), reverse=True))\n",
    "        return tuple(users[i:i+batch_size]\n",
    "                for i in range(0, len(users), batch_size))\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path):\n",
    "        data = collections.defaultdict(list)\n",
    "        num_users = 0\n",
    "        num_items = 0\n",
    "        num_tags = 0\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                u, i, t = map(int, line.strip().split())\n",
    "                num_users = max(u, num_users)  # Users are numbered 1 -> N.\n",
    "                num_items = max(i, num_items)  # Items are numbered 1 -> M.\n",
    "                num_tags = max(t, num_tags) # Tags are numbered 1 -> T\n",
    "                data[u].append((i,t))\n",
    "        sequence = dict()\n",
    "        for user in range(1, num_users + 1):\n",
    "            if user in data:\n",
    "                sequence[user] = np.array(data[user])\n",
    "            else:\n",
    "                sequence[user] = np.array([[0,0]])\n",
    "        return cls(num_users, num_items, num_tags, sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CGRUCell(tf.contrib.rnn.RNNCell):\n",
    "\n",
    "    def __init__(self, num_units, num_users, num_items, num_tags):\n",
    "        \"\"\"Note: users are numbered 1 to N, items are numbered 1 to M. User and\n",
    "        item \"zero\" is reserved for padding purposes.\n",
    "        \"\"\"\n",
    "        self._num_units = num_units\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # shape(inputs) = [batch_size, input_size]\n",
    "        # shape(state) = [batch_size, num_units]\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"CollaborativeGRUCell\"\n",
    "            with tf.variable_scope(\"Gates\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, 2 * num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, 2 * num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    #tags = tf.get_variable(\"tags\",\n",
    "                    #        [self._num_tags + 1, 2 * self._num_units],\n",
    "                    #        dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, 2 * num_units]\n",
    "                    #w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(state, 1), w_hidden_u)\n",
    "                res = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i)# + w_input_t)\n",
    "                r, z = tf.split(axis=1, num_or_size_splits=2, value=res)\n",
    "            with tf.variable_scope(\"Candidate\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    #tags = tf.get_variable(\"tags\",\n",
    "                    #        [self._num_tags + 1, self._num_units],\n",
    "                    #        dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, num_units]\n",
    "                    #w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(r * state, 1), w_hidden_u)\n",
    "                # check with tanh and relu\n",
    "                c = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i)# + w_input_t)\n",
    "            new_h = z * state + (1 - z) * c\n",
    "        return new_h, new_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-loss: 8.452 speed: 1948 wps\n",
      "log-loss: 8.421 speed: 2168 wps\n",
      "log-loss: 8.356 speed: 2196 wps\n",
      "log-loss: 8.344 speed: 2262 wps\n",
      "log-loss: 8.351 speed: 2275 wps\n",
      "log-loss: 8.299 speed: 2270 wps\n",
      "log-loss: 8.283 speed: 2266 wps\n",
      "log-loss: 8.262 speed: 2312 wps\n",
      "log-loss: 8.276 speed: 2311 wps\n",
      "Epoch 1, train log-loss: 8.265, train eff: 0.01317677400606622\n",
      "Epoch 1, valid log-loss: 7.706, valid eff: 0.014844402050792894\n",
      "log-loss: 6.835 speed: 1965 wps\n",
      "log-loss: 6.996 speed: 2143 wps\n",
      "log-loss: 6.979 speed: 2156 wps\n",
      "log-loss: 6.974 speed: 2166 wps\n",
      "log-loss: 6.964 speed: 2134 wps\n",
      "log-loss: 6.959 speed: 2241 wps\n",
      "log-loss: 6.964 speed: 2279 wps\n",
      "log-loss: 6.933 speed: 2309 wps\n",
      "log-loss: 6.903 speed: 2254 wps\n",
      "Epoch 2, train log-loss: 6.890, train eff: 0.07363010588641276\n",
      "Epoch 2, valid log-loss: 7.088, valid eff: 0.039495648026708\n",
      "log-loss: 6.034 speed: 2617 wps\n",
      "log-loss: 5.752 speed: 2388 wps\n",
      "log-loss: 5.729 speed: 2494 wps\n",
      "log-loss: 5.680 speed: 2393 wps\n",
      "log-loss: 5.586 speed: 2371 wps\n",
      "log-loss: 5.589 speed: 2363 wps\n",
      "log-loss: 5.570 speed: 2241 wps\n",
      "log-loss: 5.594 speed: 2245 wps\n",
      "log-loss: 5.584 speed: 2219 wps\n",
      "Epoch 3, train log-loss: 5.568, train eff: 0.13267585680815125\n",
      "Epoch 3, valid log-loss: 6.736, valid eff: 0.07153928699177298\n",
      "log-loss: 4.481 speed: 1669 wps\n",
      "log-loss: 4.501 speed: 2070 wps\n",
      "log-loss: 4.528 speed: 2071 wps\n",
      "log-loss: 4.433 speed: 2047 wps\n",
      "log-loss: 4.446 speed: 2143 wps\n",
      "log-loss: 4.457 speed: 2180 wps\n",
      "log-loss: 4.474 speed: 2191 wps\n",
      "log-loss: 4.451 speed: 2147 wps\n",
      "log-loss: 4.471 speed: 2193 wps\n",
      "Epoch 4, train log-loss: 4.471, train eff: 0.20695072558528124\n",
      "Epoch 4, valid log-loss: 6.612, valid eff: 0.10635507332776917\n",
      "log-loss: 3.605 speed: 2000 wps\n",
      "log-loss: 3.700 speed: 2325 wps\n",
      "log-loss: 3.787 speed: 2253 wps\n",
      "log-loss: 3.733 speed: 2230 wps\n",
      "log-loss: 3.623 speed: 2088 wps\n",
      "log-loss: 3.661 speed: 2052 wps\n",
      "log-loss: 3.683 speed: 2099 wps\n",
      "log-loss: 3.715 speed: 2155 wps\n",
      "log-loss: 3.721 speed: 2190 wps\n",
      "Epoch 5, train log-loss: 3.726, train eff: 0.26764924898744985\n",
      "Epoch 5, valid log-loss: 6.758, valid eff: 0.1340765470370812\n",
      "log-loss: 3.195 speed: 2405 wps\n",
      "log-loss: 3.390 speed: 2247 wps\n",
      "log-loss: 3.383 speed: 2265 wps\n",
      "log-loss: 3.282 speed: 2190 wps\n",
      "log-loss: 3.289 speed: 2238 wps\n",
      "log-loss: 3.297 speed: 2295 wps\n",
      "log-loss: 3.271 speed: 2263 wps\n",
      "log-loss: 3.282 speed: 2291 wps\n",
      "log-loss: 3.235 speed: 2203 wps\n",
      "Epoch 6, train log-loss: 3.223, train eff: 0.31425380046858825\n",
      "Epoch 6, valid log-loss: 6.926, valid eff: 0.151991176821271\n",
      "log-loss: 3.159 speed: 2395 wps\n",
      "log-loss: 2.961 speed: 2257 wps\n",
      "log-loss: 3.010 speed: 2331 wps\n",
      "log-loss: 2.922 speed: 2226 wps\n",
      "log-loss: 2.891 speed: 2159 wps\n",
      "log-loss: 2.831 speed: 2095 wps\n",
      "log-loss: 2.842 speed: 2154 wps\n",
      "log-loss: 2.856 speed: 2128 wps\n",
      "log-loss: 2.871 speed: 2134 wps\n",
      "Epoch 7, train log-loss: 2.860, train eff: 0.3542835866979059\n",
      "Epoch 7, valid log-loss: 7.010, valid eff: 0.16510671276976271\n",
      "log-loss: 2.540 speed: 2149 wps\n",
      "log-loss: 2.583 speed: 2432 wps\n",
      "log-loss: 2.571 speed: 2382 wps\n",
      "log-loss: 2.516 speed: 2252 wps\n",
      "log-loss: 2.512 speed: 2285 wps\n",
      "log-loss: 2.501 speed: 2233 wps\n",
      "log-loss: 2.538 speed: 2271 wps\n",
      "log-loss: 2.548 speed: 2211 wps\n",
      "log-loss: 2.565 speed: 2212 wps\n",
      "Epoch 8, train log-loss: 2.562, train eff: 0.3911258831435369\n",
      "Epoch 8, valid log-loss: 7.142, valid eff: 0.1741087397162275\n",
      "log-loss: 2.393 speed: 2477 wps\n",
      "log-loss: 2.307 speed: 2305 wps\n",
      "log-loss: 2.313 speed: 2316 wps\n",
      "log-loss: 2.322 speed: 2326 wps\n",
      "log-loss: 2.339 speed: 2386 wps\n",
      "log-loss: 2.354 speed: 2357 wps\n",
      "log-loss: 2.341 speed: 2341 wps\n",
      "log-loss: 2.348 speed: 2288 wps\n",
      "log-loss: 2.317 speed: 2259 wps\n",
      "Epoch 9, train log-loss: 2.305, train eff: 0.4230098621478777\n",
      "Epoch 9, valid log-loss: 7.306, valid eff: 0.17500298080362467\n",
      "log-loss: 2.094 speed: 2137 wps\n",
      "log-loss: 2.180 speed: 2215 wps\n",
      "log-loss: 2.107 speed: 2153 wps\n",
      "log-loss: 2.117 speed: 2167 wps\n",
      "log-loss: 2.137 speed: 2175 wps\n",
      "log-loss: 2.140 speed: 2147 wps\n",
      "log-loss: 2.123 speed: 2159 wps\n",
      "log-loss: 2.128 speed: 2199 wps\n",
      "log-loss: 2.122 speed: 2211 wps\n",
      "Epoch 10, train log-loss: 2.111, train eff: 0.45025336457254944\n",
      "Epoch 10, valid log-loss: 7.402, valid eff: 0.17392989149874807\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEXCAYAAABCjVgAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f12e75ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CollaborativeRecc(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, is_training,\n",
    "            chunk_size=128, batch_size=1, hidden_size=128,\n",
    "            learning_rate=0.1, rho=0.9):\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # placeholders for input data\n",
    "        self._inputs = tf.placeholder(tf.int32, name=\"inputs\",\n",
    "                shape=[batch_size, chunk_size, 3])\n",
    "        self._targets = tf.placeholder(tf.int32, name=\"targets\",\n",
    "                shape=[batch_size, chunk_size])\n",
    "        self._seq_length = tf.placeholder(tf.int32, name=\"seq_length\",\n",
    "                shape=[batch_size])\n",
    "\n",
    "        # RNN cell.\n",
    "        cell = CGRUCell(hidden_size, num_users, num_items, num_tags)\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_\n",
    "                in tf.split(axis=1, num_or_size_splits=chunk_size, value=self._inputs)]\n",
    "        #[O]states, _ = tf.nn.rnn(cell, inputs,\n",
    "        #        initial_state=self._initial_state)\n",
    "\n",
    "        states, _ = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                initial_state=self._initial_state)      #[N]\n",
    "\n",
    "        # Compute the final state for each element of the batch.\n",
    "        self._final_state = tf.gather_nd([self._initial_state] + states,\n",
    "                tf.transpose(tf.stack([self._seq_length, tf.range(batch_size)])))\n",
    "\n",
    "        # Output layer.\n",
    "        # `output` has shape (batch_size * chunk_size, hidden_size).\n",
    "        output = tf.reshape(tf.concat(axis=1, values=states), [-1, hidden_size])\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            ws = tf.get_variable(\"weights\", [hidden_size, num_items + 1],\n",
    "                                 dtype=tf.float32)\n",
    "        # `logits` has shape (batch_size * chunk_size, num_items).\n",
    "        logits = tf.matmul(output, ws)\n",
    "        targets = tf.reshape(self._targets, [-1])\n",
    "        \n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        self._predicts = tf.argmax(\n",
    "            tf.reshape(predictions, [batch_size,chunk_size,num_items+1]), 2)\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "\n",
    "        masked = loss * tf.to_float(tf.sign(targets))\n",
    "        masked = tf.reshape(masked, [batch_size, chunk_size])\n",
    "        self._cost = tf.reduce_sum(masked, axis=1)\n",
    "\n",
    "        if not is_training:\n",
    "            self._train_op = tf.no_op()\n",
    "            return\n",
    "\n",
    "        scalar_cost = tf.reduce_mean(masked)\n",
    "\n",
    "        # Optimization procedure.\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate, decay=rho, epsilon=1e-8)\n",
    "        self._train_op = optimizer.minimize(scalar_cost)\n",
    "        \n",
    "        self._rms_reset = list()\n",
    "        for var in tf.trainable_variables():\n",
    "            slot = optimizer.get_slot(var, \"rms\")\n",
    "            op = slot.assign(tf.zeros(slot.get_shape()))\n",
    "            self._rms_reset.append(op)\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def seq_length(self):\n",
    "        return self._seq_length\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._predicts\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def rms_reset(self):\n",
    "        return self._rms_reset\n",
    "\n",
    "\n",
    "def run_batch(session, model, iterator, initial_state):\n",
    "    \"\"\"Runs the model on all chunks of one batch.\"\"\"\n",
    "    costs = np.zeros(model.batch_size)\n",
    "    sizes = np.zeros(model.batch_size)\n",
    "    correct = 0\n",
    "    state = initial_state\n",
    "    for inputs, targets, seq_len in iterator:\n",
    "        fetches = [model.cost, model.outputs, model.final_state, model.train_op]\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.inputs] = inputs\n",
    "        feed_dict[model.targets] = targets\n",
    "        feed_dict[model.seq_length] = seq_len\n",
    "        feed_dict[model.initial_state] = state\n",
    "        cost, outputs, state, _ = session.run(fetches, feed_dict)\n",
    "        costs += cost\n",
    "        sizes += seq_len\n",
    "        correct += np.sum(outputs == targets)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        errors = costs / sizes\n",
    "    return (errors, np.sum(sizes), state, correct)\n",
    "\n",
    "\n",
    "def run_epoch(session, train_model, valid_model, train_iter, valid_iter,\n",
    "        tot_size):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_errors = list()\n",
    "    valid_errors = list()\n",
    "    tot = 0\n",
    "    train_correct = 0\n",
    "    train_seq_len = 0\n",
    "    valid_correct = 0\n",
    "    valid_seq_len = 0\n",
    "\n",
    "    next_tenth = tot_size / 10\n",
    "\n",
    "    for train, valid in zip(train_iter, valid_iter):\n",
    "        state = session.run(train_model.initial_state)\n",
    "        # Training data.\n",
    "        errors, num_triplets, state, correct = run_batch(\n",
    "                session, train_model, train, state)\n",
    "        tot += num_triplets\n",
    "        train_seq_len += num_triplets\n",
    "        train_correct += correct\n",
    "        train_errors.extend(errors)\n",
    "        # Validation data.\n",
    "        errors, num_triplets, state, correct = run_batch(\n",
    "                session, valid_model, valid, state)\n",
    "        tot += num_triplets\n",
    "        valid_seq_len += num_triplets\n",
    "        valid_correct += correct\n",
    "        valid_errors.extend(errors)\n",
    "\n",
    "        if tot > next_tenth:\n",
    "            print(\"log-loss: {:.3f} speed: {:.0f} wps\".format(\n",
    "                    np.nanmean(train_errors),\n",
    "                    tot / (time.time() - start_time)))\n",
    "            next_tenth += tot_size / 10\n",
    "            \n",
    "        train_eff = train_correct/train_seq_len\n",
    "        valid_eff = valid_correct/valid_seq_len\n",
    "\n",
    "    return (np.nanmean(train_errors), np.nanmean(valid_errors),\n",
    "           train_eff, valid_eff)\n",
    "\n",
    "def main(train_path,valid_path,bs=5,cs=64,hs=128,lr=0.01,\n",
    "         epochs=10,rho=0.9):\n",
    "\n",
    "    train_data = Dataset.from_path(train_path)\n",
    "    valid_data = Dataset.from_path(valid_path)\n",
    "    \n",
    "    num_users = train_data.num_users\n",
    "    num_items = train_data.num_items\n",
    "    num_tags = train_data.num_tags\n",
    "    tot_size = train_data.num_triplets + valid_data.num_triplets\n",
    "\n",
    "    train_data.prepare_batches(cs, bs)\n",
    "    valid_data.prepare_batches(cs, bs, batches_like=train_data)\n",
    "\n",
    "    settings = {\n",
    "        \"chunk_size\": cs,\n",
    "        \"batch_size\": bs,\n",
    "        \"hidden_size\": hs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"rho\": rho,\n",
    "    }\n",
    "    \n",
    "    train_loss = list()\n",
    "    valid_loss = list()\n",
    "    _train_eff = list()\n",
    "    _valid_eff = list()\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        initializer = tf.random_normal_initializer(\n",
    "                mean=0, stddev=1/sqrt(hs))\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            train_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=True, **settings)\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            valid_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=False, **settings)\n",
    "        tf.global_variables_initializer().run()\n",
    "        session.run(train_model.rms_reset)\n",
    "        for i in range(1, epochs + 1):\n",
    "            order = np.random.permutation(train_data.num_batches)\n",
    "            train_iter = train_data.iter_batches(order=order)\n",
    "            valid_iter = valid_data.iter_batches(order=order)\n",
    "\n",
    "            train_err, valid_err, train_eff, valid_eff = run_epoch(session, train_model, valid_model,\n",
    "                    train_iter, valid_iter, tot_size)\n",
    "            train_loss.append(train_err)\n",
    "            valid_loss.append(valid_err)\n",
    "            _train_eff.append(train_eff)\n",
    "            _valid_eff.append(valid_eff)\n",
    "            \n",
    "            print(\"Epoch {}, train log-loss: {:.3f}, train eff: {}\".format(i, train_err, train_eff))\n",
    "            print(\"Epoch {}, valid log-loss: {:.3f}, valid eff: {}\".format(i, valid_err, valid_eff))\n",
    "     \n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2)\n",
    "\n",
    "    ax0.plot(np.arange(1,epochs+1), train_loss, 'o-')\n",
    "    ax0.plot(np.arange(1,epochs+1), valid_loss, '.-')\n",
    "    ax0.set(xlabel='Epochs', ylabel='Mean Loss (cross-entropy)',\n",
    "       title='Train and Valid loss per Epoch')\n",
    "    \n",
    "    ax1.plot(np.arange(1,epochs+1), _train_eff, 'o-')\n",
    "    ax1.plot(np.arange(1,epochs+1), _valid_eff, '.-')\n",
    "    ax1.set(xlabel='Epochs', ylabel='Efficiency (/100)',\n",
    "       title='Efficiency(/100) per Epoch')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "args = {\n",
    "    \"train_path\" : train_file,\n",
    "    \"valid_path\" : valid_file,\n",
    "    \"bs\" : 5,  # batch size\n",
    "    \"cs\" : 64,  # chunk size\n",
    "    \"hs\" : 128,  # hidden size\n",
    "    \"lr\" : 0.01,  # learning rate\n",
    "    \"epochs\" : 10,\n",
    "    \"rho\" : 0.95,  # RMSProp decay coefficient\n",
    "}\n",
    "\n",
    "main(**args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
