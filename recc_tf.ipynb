{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import datetime\n",
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from math import ceil,sqrt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"C:/Users/Jazzy/Academia/ML/lastfm/user_taggedartists-timestamps.dat\"\n",
    "train_file = \"train.txt\"\n",
    "valid_file = \"valid.txt\"\n",
    "\n",
    "ENTROPY_CUTOFF = 3.0\n",
    "\n",
    "MIN_OCCURRENCES = 10\n",
    "MIN_VALID_SEQ_LEN = 4\n",
    "#MAX_VALID_SEQ_LEN = 500\n",
    "\n",
    "def processdata():\n",
    "    data = list()\n",
    "    i_occ = collections.defaultdict(lambda: 0)\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            uid, aid, tid, ts = line.strip().split('\\t')\n",
    "            data.append((uid,aid,tid,ts))\n",
    "            i_occ[aid] += 1\n",
    "        \n",
    "    tmp_data = collections.defaultdict(list)\n",
    "    for uid,aid,tid,ts in data:\n",
    "        if i_occ[aid] > MIN_OCCURRENCES:\n",
    "            tmp_data[uid].append((ts,aid,tid))\n",
    "            \n",
    "    seq_data = dict()\n",
    "    for uid in tmp_data:\n",
    "        seq = [(aid,tid) for ts,aid,tid in sorted(tmp_data[uid])]\n",
    "        seq_data[uid] = seq\n",
    "        \n",
    "    train = dict()\n",
    "    valid = dict()\n",
    "    for user,seq in seq_data.items():\n",
    "        if len(seq) > MIN_OCCURRENCES:\n",
    "            cutoff = max(MIN_VALID_SEQ_LEN, int(round(0.25 * len(seq))))\n",
    "            train[user] = seq[:-cutoff]\n",
    "            valid[user] = seq[-cutoff:]\n",
    "          \n",
    "    items = list()\n",
    "    tags = list()\n",
    "    for x,y in itertools.chain(*train.values()):\n",
    "        items.append(x)\n",
    "        tags.append(y)\n",
    "    items = set(items)\n",
    "    tags = set(tags)\n",
    "    users = set(train.keys())\n",
    "    user2id = dict(zip(users, range(1,len(users)+1)))\n",
    "    items2id = dict(zip(items, range(1,len(items)+1)))\n",
    "    tags2id = dict(zip(tags, range(1,len(tags)+1)))\n",
    "    train_data = dict()\n",
    "    valid_data = dict()\n",
    "    \n",
    "    for user in users:\n",
    "        train_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]), train[user]))\n",
    "        valid_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]),\n",
    "                                             filter(lambda x: x[0] in items and x[1] in tags, valid[user])))\n",
    "        \n",
    "    with open(train_file, \"w\") as t, open(valid_file, \"w\") as v:\n",
    "        for uid in user2id.values():\n",
    "            for aid,tid in train_data[uid]:\n",
    "                t.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "            for aid,tid in valid_data[uid]:\n",
    "                v.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "                \n",
    "\n",
    "processdata()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, seq_dict):\n",
    "        self._seq_dict = seq_dict\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "        # These variables are set after calling `prepare_batches`.\n",
    "        self._users_in_batches = None\n",
    "        self._batches = None\n",
    "        self._seq_lengths = None\n",
    "        self._chunk_size = None\n",
    "\n",
    "    @property\n",
    "    def num_users(self):\n",
    "        return self._num_users\n",
    "\n",
    "    @property\n",
    "    def num_items(self):\n",
    "        return self._num_items\n",
    "    \n",
    "    @property\n",
    "    def num_tags(self):\n",
    "        return self._num_tags\n",
    "\n",
    "    @property\n",
    "    def num_triplets(self):\n",
    "        return sum(len(seq) for u, seq in self.sequences())\n",
    "\n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return len(self._batches)\n",
    "\n",
    "    @property\n",
    "    def users_in_batches(self):\n",
    "        if self._users_in_batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return self._users_in_batches\n",
    "    \n",
    "    def __getitem__(self, u):\n",
    "        return self._seq_dict[u]\n",
    "\n",
    "    def sequences(self):\n",
    "        return self._seq_dict.items()\n",
    "\n",
    "    def iter_batches(self, order=None):\n",
    "        if order is None:\n",
    "            order = range(self.num_batches)\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        cs = self._chunk_size\n",
    "        def iter_batch(batch, seq_length):\n",
    "            num_cols = batch.shape[1]\n",
    "            for i, z in enumerate(range(0, num_cols - 1, cs)):\n",
    "                inputs = batch[:,z:z+cs,:]\n",
    "                targets = batch[:,(z+1):(z+cs+1),1]\n",
    "                yield (inputs, targets, seq_length[:,i])\n",
    "        for i in order:\n",
    "            yield iter_batch(self._batches[i], self._seq_lengths[i])\n",
    "\n",
    "    def prepare_batches(self, chunk_size, batch_size, batches_like=None):\n",
    "        # Spread users over batches.\n",
    "        if batches_like is not None:\n",
    "            self._users_in_batches = batches_like.users_in_batches\n",
    "        else:\n",
    "            self._users_in_batches = Dataset._assign_users_to_batches(\n",
    "                    batch_size, self._seq_dict)\n",
    "        # Build the batches and record the corresponding valid sequence lengths.\n",
    "        self._chunk_size = chunk_size\n",
    "        self._batches = list()\n",
    "        self._seq_lengths = list()\n",
    "        for users in self._users_in_batches:\n",
    "#           print(\"users :\",users)\n",
    "            lengths = tuple(len(self[u]) for u in users)\n",
    "#           print(\"lengths: \",lengths)\n",
    "            num_chunks = int(ceil(max(max(lengths) - 1, chunk_size)\n",
    "                    / chunk_size))\n",
    "            num_cols = num_chunks * chunk_size + 1\n",
    "#           print(\"number of columns: \",num_cols)\n",
    "            batch = np.zeros((batch_size, num_cols, 3), dtype=np.int32)\n",
    "            seq_length = np.zeros((batch_size, num_chunks), dtype=np.int32)\n",
    "            for i, (user, length) in enumerate(zip(users, lengths)):\n",
    "                # Assign the values to the batch.\n",
    "                batch[i,:length,0] = user\n",
    "                batch[i,:length,1:] = self[user]\n",
    "#               print(\"self[user]\",user,self[user])\n",
    "                # Compute and assign the valid sequence lengths.\n",
    "                q, r = divmod(max(0, min(num_cols, length) - 1), chunk_size)\n",
    "                seq_length[i,:q] = chunk_size\n",
    "                if r > 0:\n",
    "                    seq_length[i,q] = r\n",
    "#           print(\"batch: \", batch)\n",
    "            self._batches.append(batch)\n",
    "            self._seq_lengths.append(seq_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def _assign_users_to_batches(batch_size, seq_dict):\n",
    "        lengths, users = zip(*sorted(((len(seq), u)\n",
    "                for u, seq in seq_dict.items()), reverse=True))\n",
    "        return tuple(users[i:i+batch_size]\n",
    "                for i in range(0, len(users), batch_size))\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path):\n",
    "        data = collections.defaultdict(list)\n",
    "        num_users = 0\n",
    "        num_items = 0\n",
    "        num_tags = 0\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                u, i, t = map(int, line.strip().split())\n",
    "                num_users = max(u, num_users)  # Users are numbered 1 -> N.\n",
    "                num_items = max(i, num_items)  # Items are numbered 1 -> M.\n",
    "                num_tags = max(t, num_tags) # Tags are numbered 1 -> T\n",
    "                data[u].append((i,t))\n",
    "        sequence = dict()\n",
    "        for user in range(1, num_users + 1):\n",
    "            if user in data:\n",
    "                sequence[user] = np.array(data[user])\n",
    "            else:\n",
    "                sequence[user] = np.array([[0,0]])\n",
    "        return cls(num_users, num_items, num_tags, sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CGRUCell(tf.contrib.rnn.RNNCell):\n",
    "\n",
    "    def __init__(self, num_units, num_users, num_items, num_tags):\n",
    "        \"\"\"Note: users are numbered 1 to N, items are numbered 1 to M. User and\n",
    "        item \"zero\" is reserved for padding purposes.\n",
    "        \"\"\"\n",
    "        self._num_units = num_units\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # shape(inputs) = [batch_size, input_size]\n",
    "        # shape(state) = [batch_size, num_units]\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"CollaborativeGRUCell\"\n",
    "            with tf.variable_scope(\"Gates\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, 2 * num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, 2 * num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    tags = tf.get_variable(\"tags\",\n",
    "                            [self._num_tags + 1, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, 2 * num_units]\n",
    "                    w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(state, 1), w_hidden_u)\n",
    "                res = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i + w_input_t)\n",
    "                r, z = tf.split(axis=1, num_or_size_splits=2, value=res)\n",
    "            with tf.variable_scope(\"Candidate\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    tags = tf.get_variable(\"tags\",\n",
    "                            [self._num_tags + 1, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, num_units]\n",
    "                    w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(r * state, 1), w_hidden_u)\n",
    "                # check with tanh and relu\n",
    "                c = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i + w_input_t)\n",
    "            new_h = z * state + (1 - z) * c\n",
    "        return new_h, new_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-loss: 8.364 speed: 2177 wps\n",
      "log-loss: 8.327 speed: 2482 wps\n",
      "log-loss: 8.263 speed: 2531 wps\n",
      "log-loss: 8.240 speed: 2551 wps\n",
      "log-loss: 8.198 speed: 2642 wps\n",
      "log-loss: 8.143 speed: 2613 wps\n",
      "log-loss: 8.140 speed: 2665 wps\n",
      "log-loss: 8.113 speed: 2721 wps\n",
      "log-loss: 8.106 speed: 2750 wps\n",
      "Epoch 1, train log-loss: 8.064\n",
      "Epoch 1, valid log-loss: 7.619\n",
      "log-loss: 7.179 speed: 3006 wps\n",
      "log-loss: 7.175 speed: 2835 wps\n",
      "log-loss: 6.995 speed: 2773 wps\n",
      "log-loss: 6.943 speed: 2711 wps\n",
      "log-loss: 6.901 speed: 2728 wps\n",
      "log-loss: 6.897 speed: 2772 wps\n",
      "log-loss: 6.847 speed: 2855 wps\n",
      "log-loss: 6.857 speed: 2848 wps\n",
      "log-loss: 6.790 speed: 2811 wps\n",
      "Epoch 2, train log-loss: 6.747\n",
      "Epoch 2, valid log-loss: 6.944\n",
      "log-loss: 5.349 speed: 2386 wps\n",
      "log-loss: 5.483 speed: 2522 wps\n",
      "log-loss: 5.484 speed: 2547 wps\n",
      "log-loss: 5.481 speed: 2721 wps\n",
      "log-loss: 5.528 speed: 2735 wps\n",
      "log-loss: 5.531 speed: 2768 wps\n",
      "log-loss: 5.537 speed: 2782 wps\n",
      "log-loss: 5.517 speed: 2822 wps\n",
      "log-loss: 5.516 speed: 2743 wps\n",
      "Epoch 3, train log-loss: 5.529\n",
      "Epoch 3, valid log-loss: 6.460\n",
      "log-loss: 4.806 speed: 2929 wps\n",
      "log-loss: 4.728 speed: 2790 wps\n",
      "log-loss: 4.636 speed: 2665 wps\n",
      "log-loss: 4.602 speed: 2712 wps\n",
      "log-loss: 4.617 speed: 2734 wps\n",
      "log-loss: 4.643 speed: 2827 wps\n",
      "log-loss: 4.663 speed: 2828 wps\n",
      "log-loss: 4.623 speed: 2735 wps\n",
      "log-loss: 4.584 speed: 2706 wps\n",
      "Epoch 4, train log-loss: 4.566\n",
      "Epoch 4, valid log-loss: 6.338\n",
      "log-loss: 3.636 speed: 2677 wps\n",
      "log-loss: 3.836 speed: 2911 wps\n",
      "log-loss: 3.865 speed: 2912 wps\n",
      "log-loss: 3.875 speed: 2879 wps\n",
      "log-loss: 3.861 speed: 2757 wps\n",
      "log-loss: 3.820 speed: 2676 wps\n",
      "log-loss: 3.802 speed: 2625 wps\n",
      "log-loss: 3.782 speed: 2606 wps\n",
      "log-loss: 3.775 speed: 2656 wps\n",
      "Epoch 5, train log-loss: 3.787\n",
      "Epoch 5, valid log-loss: 6.335\n",
      "log-loss: 3.262 speed: 2605 wps\n",
      "log-loss: 3.292 speed: 2494 wps\n",
      "log-loss: 3.250 speed: 2661 wps\n",
      "log-loss: 3.259 speed: 2715 wps\n",
      "log-loss: 3.267 speed: 2748 wps\n",
      "log-loss: 3.221 speed: 2708 wps\n",
      "log-loss: 3.223 speed: 2724 wps\n",
      "log-loss: 3.223 speed: 2721 wps\n",
      "log-loss: 3.223 speed: 2767 wps\n",
      "Epoch 6, train log-loss: 3.196\n",
      "Epoch 6, valid log-loss: 6.453\n",
      "log-loss: 2.711 speed: 2787 wps\n",
      "log-loss: 2.677 speed: 2766 wps\n",
      "log-loss: 2.740 speed: 2842 wps\n",
      "log-loss: 2.768 speed: 2725 wps\n",
      "log-loss: 2.811 speed: 2797 wps\n",
      "log-loss: 2.781 speed: 2664 wps\n",
      "log-loss: 2.796 speed: 2675 wps\n",
      "log-loss: 2.788 speed: 2690 wps\n",
      "log-loss: 2.773 speed: 2687 wps\n",
      "Epoch 7, train log-loss: 2.763\n",
      "Epoch 7, valid log-loss: 6.591\n",
      "log-loss: 2.628 speed: 2968 wps\n",
      "log-loss: 2.488 speed: 2681 wps\n",
      "log-loss: 2.534 speed: 2731 wps\n",
      "log-loss: 2.516 speed: 2719 wps\n",
      "log-loss: 2.495 speed: 2708 wps\n",
      "log-loss: 2.472 speed: 2731 wps\n",
      "log-loss: 2.476 speed: 2742 wps\n",
      "log-loss: 2.505 speed: 2721 wps\n",
      "log-loss: 2.495 speed: 2748 wps\n",
      "Epoch 8, train log-loss: 2.446\n",
      "Epoch 8, valid log-loss: 6.716\n",
      "log-loss: 2.070 speed: 2402 wps\n",
      "log-loss: 2.122 speed: 2442 wps\n",
      "log-loss: 2.185 speed: 2546 wps\n",
      "log-loss: 2.156 speed: 2489 wps\n",
      "log-loss: 2.165 speed: 2508 wps\n",
      "log-loss: 2.155 speed: 2541 wps\n",
      "log-loss: 2.149 speed: 2557 wps\n",
      "log-loss: 2.155 speed: 2543 wps\n",
      "log-loss: 2.150 speed: 2548 wps\n",
      "Epoch 9, train log-loss: 2.161\n",
      "Epoch 9, valid log-loss: 6.827\n",
      "log-loss: 1.881 speed: 2463 wps\n",
      "log-loss: 1.794 speed: 2274 wps\n",
      "log-loss: 1.800 speed: 2370 wps\n",
      "log-loss: 1.810 speed: 2480 wps\n",
      "log-loss: 1.842 speed: 2454 wps\n",
      "log-loss: 1.886 speed: 2524 wps\n",
      "log-loss: 1.891 speed: 2558 wps\n",
      "log-loss: 1.897 speed: 2538 wps\n",
      "log-loss: 1.909 speed: 2592 wps\n",
      "Epoch 10, train log-loss: 1.920\n",
      "Epoch 10, valid log-loss: 6.941\n"
     ]
    }
   ],
   "source": [
    "class CollaborativeRecc(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, is_training,\n",
    "            chunk_size=128, batch_size=1, hidden_size=128,\n",
    "            learning_rate=0.1, rho=0.9):\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # placeholders for input data\n",
    "        self._inputs = tf.placeholder(tf.int32, name=\"inputs\",\n",
    "                shape=[batch_size, chunk_size, 3])\n",
    "        self._targets = tf.placeholder(tf.int32, name=\"targets\",\n",
    "                shape=[batch_size, chunk_size])\n",
    "        self._seq_length = tf.placeholder(tf.int32, name=\"seq_length\",\n",
    "                shape=[batch_size])\n",
    "\n",
    "        # RNN cell.\n",
    "        cell = CGRUCell(hidden_size, num_users, num_items, num_tags)\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_\n",
    "                in tf.split(axis=1, num_or_size_splits=chunk_size, value=self._inputs)]\n",
    "        #[O]states, _ = tf.nn.rnn(cell, inputs,\n",
    "        #        initial_state=self._initial_state)\n",
    "\n",
    "        states, _ = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                initial_state=self._initial_state)      #[N]\n",
    "\n",
    "        # Compute the final state for each element of the batch.\n",
    "        self._final_state = tf.gather_nd([self._initial_state] + states,\n",
    "                tf.transpose(tf.stack([self._seq_length, tf.range(batch_size)])))\n",
    "\n",
    "        # Output layer.\n",
    "        # `output` has shape (batch_size * chunk_size, hidden_size).\n",
    "        output = tf.reshape(tf.concat(axis=1, values=states), [-1, hidden_size])\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            ws = tf.get_variable(\"weights\", [hidden_size, num_items + 1],\n",
    "                                 dtype=tf.float32)\n",
    "        # `logits` has shape (batch_size * chunk_size, num_items).\n",
    "        logits = tf.matmul(output, ws)\n",
    "        targets = tf.reshape(self._targets, [-1])\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "\n",
    "        masked = loss * tf.to_float(tf.sign(targets))\n",
    "        masked = tf.reshape(masked, [batch_size, chunk_size])\n",
    "        self._cost = tf.reduce_sum(masked, axis=1)\n",
    "\n",
    "        if not is_training:\n",
    "            self._train_op = tf.no_op()\n",
    "            return\n",
    "\n",
    "        scalar_cost = tf.reduce_mean(masked)\n",
    "\n",
    "        # Optimization procedure.\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate, decay=rho, epsilon=1e-8)\n",
    "        self._train_op = optimizer.minimize(scalar_cost)\n",
    "\n",
    "        self._rms_reset = list()\n",
    "        for var in tf.trainable_variables():\n",
    "            slot = optimizer.get_slot(var, \"rms\")\n",
    "            op = slot.assign(tf.zeros(slot.get_shape()))\n",
    "            self._rms_reset.append(op)\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def seq_length(self):\n",
    "        return self._seq_length\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def rms_reset(self):\n",
    "        return self._rms_reset\n",
    "\n",
    "\n",
    "def run_batch(session, model, iterator, initial_state):\n",
    "    \"\"\"Runs the model on all chunks of one batch.\"\"\"\n",
    "    costs = np.zeros(model.batch_size)\n",
    "    sizes = np.zeros(model.batch_size)\n",
    "    state = initial_state\n",
    "    for inputs, targets, seq_len in iterator:\n",
    "        fetches = [model.cost, model.final_state, model.train_op]\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.inputs] = inputs\n",
    "        feed_dict[model.targets] = targets\n",
    "        feed_dict[model.seq_length] = seq_len\n",
    "        feed_dict[model.initial_state] = state\n",
    "        cost, state, _ = session.run(fetches, feed_dict)\n",
    "        costs += cost\n",
    "        sizes += seq_len\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        errors = costs / sizes\n",
    "    return (errors, np.sum(sizes), state)\n",
    "\n",
    "\n",
    "def run_epoch(session, train_model, valid_model, train_iter, valid_iter,\n",
    "        tot_size):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_errors = list()\n",
    "    valid_errors = list()\n",
    "    tot = 0\n",
    "\n",
    "    next_tenth = tot_size / 10\n",
    "\n",
    "    for train, valid in zip(train_iter, valid_iter):\n",
    "        state = session.run(train_model.initial_state)\n",
    "        # Training data.\n",
    "        errors, num_triplets, state = run_batch(\n",
    "                session, train_model, train, state)\n",
    "        tot += num_triplets\n",
    "        train_errors.extend(errors)\n",
    "        # Validation data.\n",
    "        errors, num_triplets, state = run_batch(\n",
    "                session, valid_model, valid, state)\n",
    "        tot += num_triplets\n",
    "        valid_errors.extend(errors)\n",
    "\n",
    "        if tot > next_tenth:\n",
    "            print(\"log-loss: {:.3f} speed: {:.0f} wps\".format(\n",
    "                    np.nanmean(train_errors),\n",
    "                    tot / (time.time() - start_time)))\n",
    "            next_tenth += tot_size / 10\n",
    "\n",
    "    return (np.nanmean(train_errors), np.nanmean(valid_errors))\n",
    "\n",
    "def main(train_path,valid_path,bs=5,cs=64,hs=128,lr=0.01,\n",
    "         epochs=10,rho=0.9):\n",
    "\n",
    "    train_data = Dataset.from_path(train_path)\n",
    "    valid_data = Dataset.from_path(valid_path)\n",
    "    \n",
    "    num_users = train_data.num_users\n",
    "    num_items = train_data.num_items\n",
    "    num_tags = train_data.num_tags\n",
    "    tot_size = train_data.num_triplets + valid_data.num_triplets\n",
    "\n",
    "    train_data.prepare_batches(cs, bs)\n",
    "    valid_data.prepare_batches(cs, bs, batches_like=train_data)\n",
    "\n",
    "    settings = {\n",
    "        \"chunk_size\": cs,\n",
    "        \"batch_size\": bs,\n",
    "        \"hidden_size\": hs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"rho\": rho,\n",
    "    }\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        initializer = tf.random_normal_initializer(\n",
    "                mean=0, stddev=1/sqrt(hs))\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            train_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=True, **settings)\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            valid_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=False, **settings)\n",
    "        tf.global_variables_initializer().run()\n",
    "        session.run(train_model.rms_reset)\n",
    "        for i in range(1, epochs + 1):\n",
    "            order = np.random.permutation(train_data.num_batches)\n",
    "            train_iter = train_data.iter_batches(order=order)\n",
    "            valid_iter = valid_data.iter_batches(order=order)\n",
    "\n",
    "            train_err, valid_err = run_epoch(session, train_model, valid_model,\n",
    "                    train_iter, valid_iter, tot_size)\n",
    "            print(\"Epoch {}, train log-loss: {:.3f}\".format(i, train_err))\n",
    "            print(\"Epoch {}, valid log-loss: {:.3f}\".format(i, valid_err))\n",
    "\n",
    "args = {\n",
    "    \"train_path\" : train_file,\n",
    "    \"valid_path\" : valid_file,\n",
    "    \"bs\" : 5,  # batch size\n",
    "    \"cs\" : 64,  # chunk size\n",
    "    \"hs\" : 128,  # hidden size\n",
    "    \"lr\" : 0.01,  # learning rate\n",
    "    \"epochs\" : 10,\n",
    "    \"rho\" : 0.9,  # RMSProp decay coefficient\n",
    "}\n",
    "\n",
    "main(**args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
