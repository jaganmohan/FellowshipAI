{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import datetime\n",
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil,sqrt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"C:/Users/Jazzy/Academia/ML/lastfm/user_taggedartists-timestamps.dat\"\n",
    "train_file = \"train.txt\"\n",
    "valid_file = \"valid.txt\"\n",
    "\n",
    "MIN_OCCURRENCES = 10\n",
    "MIN_VALID_SEQ_LEN = 4\n",
    "\n",
    "def processdata():\n",
    "    data = list()\n",
    "    i_occ = collections.defaultdict(lambda: 0)\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            uid, aid, tid, ts = line.strip().split('\\t')\n",
    "            data.append((uid,aid,tid,ts))\n",
    "            i_occ[aid] += 1\n",
    "        \n",
    "    tmp_data = collections.defaultdict(list)\n",
    "    for uid,aid,tid,ts in data:\n",
    "        if i_occ[aid] > MIN_OCCURRENCES:\n",
    "            tmp_data[uid].append((ts,aid,tid))\n",
    "            \n",
    "    seq_data = dict()\n",
    "    for uid in tmp_data:\n",
    "        seq = [(aid,tid) for ts,aid,tid in sorted(tmp_data[uid])]\n",
    "        seq_data[uid] = seq\n",
    "        \n",
    "    train = dict()\n",
    "    valid = dict()\n",
    "    for user,seq in seq_data.items():\n",
    "        if len(seq) > MIN_OCCURRENCES:\n",
    "            cutoff = max(MIN_VALID_SEQ_LEN, int(round(0.25 * len(seq))))\n",
    "            train[user] = seq[:-cutoff]\n",
    "            valid[user] = seq[-cutoff:]\n",
    "          \n",
    "    items = list()\n",
    "    tags = list()\n",
    "    for x,y in itertools.chain(*train.values()):\n",
    "        items.append(x)\n",
    "        tags.append(y)\n",
    "    items = set(items)\n",
    "    tags = set(tags)\n",
    "    users = set(train.keys())\n",
    "    user2id = dict(zip(users, range(1,len(users)+1)))\n",
    "    items2id = dict(zip(items, range(1,len(items)+1)))\n",
    "    tags2id = dict(zip(tags, range(1,len(tags)+1)))\n",
    "    train_data = dict()\n",
    "    valid_data = dict()\n",
    "    \n",
    "    for user in users:\n",
    "        train_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]), train[user]))\n",
    "        valid_data[user2id[user]] = tuple(map(lambda x: (items2id[x[0]],tags2id[x[1]]),\n",
    "                                             filter(lambda x: x[0] in items and x[1] in tags, valid[user])))\n",
    "        \n",
    "    with open(train_file, \"w\") as t, open(valid_file, \"w\") as v:\n",
    "        for uid in user2id.values():\n",
    "            for aid,tid in train_data[uid]:\n",
    "                t.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "            for aid,tid in valid_data[uid]:\n",
    "                v.write(\"{} {} {}\\n\".format(uid, aid, tid))\n",
    "                \n",
    "\n",
    "processdata()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, seq_dict):\n",
    "        self._seq_dict = seq_dict\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "        # These variables are set after calling `prepare_batches`.\n",
    "        self._users_in_batches = None\n",
    "        self._batches = None\n",
    "        self._seq_lengths = None\n",
    "        self._chunk_size = None\n",
    "\n",
    "    @property\n",
    "    def num_users(self):\n",
    "        return self._num_users\n",
    "\n",
    "    @property\n",
    "    def num_items(self):\n",
    "        return self._num_items\n",
    "    \n",
    "    @property\n",
    "    def num_tags(self):\n",
    "        return self._num_tags\n",
    "\n",
    "    @property\n",
    "    def num_triplets(self):\n",
    "        return sum(len(seq) for u, seq in self.sequences())\n",
    "\n",
    "    @property\n",
    "    def num_batches(self):\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return len(self._batches)\n",
    "\n",
    "    @property\n",
    "    def users_in_batches(self):\n",
    "        if self._users_in_batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        return self._users_in_batches\n",
    "    \n",
    "    def __getitem__(self, u):\n",
    "        return self._seq_dict[u]\n",
    "\n",
    "    def sequences(self):\n",
    "        return self._seq_dict.items()\n",
    "\n",
    "    def iter_batches(self, order=None):\n",
    "        if order is None:\n",
    "            order = range(self.num_batches)\n",
    "        if self._batches is None:\n",
    "            raise RuntimeError(\"`prepare_batches` has not been called yet.\")\n",
    "        cs = self._chunk_size\n",
    "        def iter_batch(batch, seq_length):\n",
    "            num_cols = batch.shape[1]\n",
    "            for i, z in enumerate(range(0, num_cols - 1, cs)):\n",
    "                inputs = batch[:,z:z+cs,:]\n",
    "                targets = batch[:,(z+1):(z+cs+1),1]\n",
    "                yield (inputs, targets, seq_length[:,i])\n",
    "        for i in order:\n",
    "            yield iter_batch(self._batches[i], self._seq_lengths[i])\n",
    "\n",
    "    def prepare_batches(self, chunk_size, batch_size, batches_like=None):\n",
    "        # Spread users over batches.\n",
    "        if batches_like is not None:\n",
    "            self._users_in_batches = batches_like.users_in_batches\n",
    "        else:\n",
    "            self._users_in_batches = Dataset._assign_users_to_batches(\n",
    "                    batch_size, self._seq_dict)\n",
    "        # Build the batches and record the corresponding valid sequence lengths.\n",
    "        self._chunk_size = chunk_size\n",
    "        self._batches = list()\n",
    "        self._seq_lengths = list()\n",
    "        for users in self._users_in_batches:\n",
    "#           print(\"users :\",users)\n",
    "            lengths = tuple(len(self[u]) for u in users)\n",
    "#           print(\"lengths: \",lengths)\n",
    "            num_chunks = int(ceil(max(max(lengths) - 1, chunk_size)\n",
    "                    / chunk_size))\n",
    "            num_cols = num_chunks * chunk_size + 1\n",
    "#           print(\"number of columns: \",num_cols)\n",
    "            batch = np.zeros((batch_size, num_cols, 3), dtype=np.int32)\n",
    "            seq_length = np.zeros((batch_size, num_chunks), dtype=np.int32)\n",
    "            for i, (user, length) in enumerate(zip(users, lengths)):\n",
    "                # Assign the values to the batch.\n",
    "                batch[i,:length,0] = user\n",
    "                batch[i,:length,1:] = self[user]\n",
    "#               print(\"self[user]\",user,self[user])\n",
    "                # Compute and assign the valid sequence lengths.\n",
    "                q, r = divmod(max(0, min(num_cols, length) - 1), chunk_size)\n",
    "                seq_length[i,:q] = chunk_size\n",
    "                if r > 0:\n",
    "                    seq_length[i,q] = r\n",
    "#           print(\"batch: \", batch)\n",
    "            self._batches.append(batch)\n",
    "            self._seq_lengths.append(seq_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def _assign_users_to_batches(batch_size, seq_dict):\n",
    "        lengths, users = zip(*sorted(((len(seq), u)\n",
    "                for u, seq in seq_dict.items()), reverse=True))\n",
    "        return tuple(users[i:i+batch_size]\n",
    "                for i in range(0, len(users), batch_size))\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path):\n",
    "        data = collections.defaultdict(list)\n",
    "        num_users = 0\n",
    "        num_items = 0\n",
    "        num_tags = 0\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                u, i, t = map(int, line.strip().split())\n",
    "                num_users = max(u, num_users)  # Users are numbered 1 -> N.\n",
    "                num_items = max(i, num_items)  # Items are numbered 1 -> M.\n",
    "                num_tags = max(t, num_tags) # Tags are numbered 1 -> T\n",
    "                data[u].append((i,t))\n",
    "        sequence = dict()\n",
    "        for user in range(1, num_users + 1):\n",
    "            if user in data:\n",
    "                sequence[user] = np.array(data[user])\n",
    "            else:\n",
    "                sequence[user] = np.array([[0,0]])\n",
    "        return cls(num_users, num_items, num_tags, sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CGRUCell(tf.contrib.rnn.RNNCell):\n",
    "\n",
    "    def __init__(self, num_units, num_users, num_items, num_tags):\n",
    "        \"\"\"Note: users are numbered 1 to N, items are numbered 1 to M. User and\n",
    "        item \"zero\" is reserved for padding purposes.\n",
    "        \"\"\"\n",
    "        self._num_units = num_units\n",
    "        self._num_users = num_users\n",
    "        self._num_items = num_items\n",
    "        self._num_tags = num_tags\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        # shape(inputs) = [batch_size, input_size]\n",
    "        # shape(state) = [batch_size, num_units]\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"CollaborativeGRUCell\"\n",
    "            with tf.variable_scope(\"Gates\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, 2 * num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, 2 * num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    tags = tf.get_variable(\"tags\",\n",
    "                            [self._num_tags + 1, 2 * self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, 2 * num_units]\n",
    "                    w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(state, 1), w_hidden_u)\n",
    "                res = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i + w_input_t)\n",
    "                r, z = tf.split(axis=1, num_or_size_splits=2, value=res)\n",
    "            with tf.variable_scope(\"Candidate\"):\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    users = tf.get_variable(\"users\",\n",
    "                            [self._num_users + 1, self._num_units, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_hidden_u) = [batch_size, num_units, num_units]\n",
    "                    w_hidden_u = tf.nn.embedding_lookup(users, inputs[:,0])\n",
    "                    items = tf.get_variable(\"items\",\n",
    "                            [self._num_items + 1, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_i) = [batch_size, num_units]\n",
    "                    w_input_i = tf.nn.embedding_lookup(items, inputs[:,1])\n",
    "                    tags = tf.get_variable(\"tags\",\n",
    "                            [self._num_tags + 1, self._num_units],\n",
    "                            dtype=tf.float32)\n",
    "                    # shape(w_input_t) = [batch_size, num_units]\n",
    "                    w_input_t = tf.nn.embedding_lookup(tags, inputs[:,2])\n",
    "                res = tf.matmul(tf.expand_dims(r * state, 1), w_hidden_u)\n",
    "                # check with tanh and relu\n",
    "                c = tf.sigmoid(tf.squeeze(res, [1]) + w_input_i + w_input_t)\n",
    "            new_h = z * state + (1 - z) * c\n",
    "        return new_h, new_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-loss: 8.353 speed: 1961 wps\n",
      "log-loss: 8.341 speed: 2078 wps\n",
      "log-loss: 8.388 speed: 2181 wps\n",
      "log-loss: 8.402 speed: 2214 wps\n",
      "log-loss: 8.351 speed: 2259 wps\n",
      "log-loss: 8.321 speed: 2333 wps\n",
      "log-loss: 8.288 speed: 2343 wps\n",
      "log-loss: 8.251 speed: 2338 wps\n",
      "log-loss: 8.187 speed: 2251 wps\n",
      "Epoch 1, train log-loss: 8.136, train eff: 0.01650956246935106\n",
      "Epoch 1, valid log-loss: 7.591, valid eff: 0.01722904495051866\n",
      "log-loss: 7.125 speed: 1861 wps\n",
      "log-loss: 7.067 speed: 2232 wps\n",
      "log-loss: 6.996 speed: 2316 wps\n",
      "log-loss: 6.969 speed: 2171 wps\n",
      "log-loss: 6.864 speed: 2228 wps\n",
      "log-loss: 6.846 speed: 2264 wps\n",
      "log-loss: 6.767 speed: 2238 wps\n",
      "log-loss: 6.762 speed: 2255 wps\n",
      "log-loss: 6.723 speed: 2268 wps\n",
      "Epoch 2, train log-loss: 6.696, train eff: 0.08348317259666903\n",
      "Epoch 2, valid log-loss: 6.928, valid eff: 0.04897460355311792\n",
      "log-loss: 5.940 speed: 2601 wps\n",
      "log-loss: 5.473 speed: 2181 wps\n",
      "log-loss: 5.416 speed: 2172 wps\n",
      "log-loss: 5.528 speed: 2141 wps\n",
      "log-loss: 5.468 speed: 2176 wps\n",
      "log-loss: 5.457 speed: 2226 wps\n",
      "log-loss: 5.448 speed: 2235 wps\n",
      "log-loss: 5.434 speed: 2184 wps\n",
      "log-loss: 5.406 speed: 2201 wps\n",
      "Epoch 3, train log-loss: 5.404, train eff: 0.1531448082965546\n",
      "Epoch 3, valid log-loss: 6.483, valid eff: 0.08072016215571719\n",
      "log-loss: 4.242 speed: 2247 wps\n",
      "log-loss: 4.259 speed: 2174 wps\n",
      "log-loss: 4.320 speed: 2245 wps\n",
      "log-loss: 4.297 speed: 2136 wps\n",
      "log-loss: 4.272 speed: 2200 wps\n",
      "log-loss: 4.288 speed: 2215 wps\n",
      "log-loss: 4.319 speed: 2259 wps\n",
      "log-loss: 4.331 speed: 2269 wps\n",
      "log-loss: 4.343 speed: 2238 wps\n",
      "Epoch 4, train log-loss: 4.341, train eff: 0.22543998256415845\n",
      "Epoch 4, valid log-loss: 6.341, valid eff: 0.11297245737450817\n",
      "log-loss: 3.578 speed: 2288 wps\n",
      "log-loss: 3.481 speed: 2171 wps\n",
      "log-loss: 3.484 speed: 2098 wps\n",
      "log-loss: 3.464 speed: 2175 wps\n",
      "log-loss: 3.451 speed: 2103 wps\n",
      "log-loss: 3.485 speed: 2170 wps\n",
      "log-loss: 3.511 speed: 2195 wps\n",
      "log-loss: 3.497 speed: 2188 wps\n",
      "log-loss: 3.501 speed: 2198 wps\n",
      "Epoch 5, train log-loss: 3.511, train eff: 0.2872464083982637\n",
      "Epoch 5, valid log-loss: 6.399, valid eff: 0.13467270776201265\n",
      "log-loss: 2.860 speed: 1673 wps\n",
      "log-loss: 2.861 speed: 1772 wps\n",
      "log-loss: 2.857 speed: 1868 wps\n",
      "log-loss: 2.868 speed: 1977 wps\n",
      "log-loss: 2.926 speed: 1998 wps\n",
      "log-loss: 2.944 speed: 2017 wps\n",
      "log-loss: 2.942 speed: 2077 wps\n",
      "log-loss: 2.951 speed: 2097 wps\n",
      "log-loss: 2.966 speed: 2111 wps\n",
      "Epoch 6, train log-loss: 2.970, train eff: 0.3353130278428595\n",
      "Epoch 6, valid log-loss: 6.591, valid eff: 0.14960653392154524\n",
      "log-loss: 3.210 speed: 2849 wps\n",
      "log-loss: 2.553 speed: 2457 wps\n",
      "log-loss: 2.631 speed: 2439 wps\n",
      "log-loss: 2.569 speed: 2228 wps\n",
      "log-loss: 2.602 speed: 2198 wps\n",
      "log-loss: 2.631 speed: 2246 wps\n",
      "log-loss: 2.630 speed: 2235 wps\n",
      "log-loss: 2.624 speed: 2234 wps\n",
      "log-loss: 2.575 speed: 2181 wps\n",
      "Epoch 7, train log-loss: 2.570, train eff: 0.381826767649249\n",
      "Epoch 7, valid log-loss: 6.660, valid eff: 0.16170859663765352\n",
      "log-loss: 2.229 speed: 2374 wps\n",
      "log-loss: 2.298 speed: 2290 wps\n",
      "log-loss: 2.315 speed: 2294 wps\n",
      "log-loss: 2.256 speed: 2238 wps\n",
      "log-loss: 2.246 speed: 2228 wps\n",
      "log-loss: 2.272 speed: 2183 wps\n",
      "log-loss: 2.290 speed: 2226 wps\n",
      "log-loss: 2.274 speed: 2238 wps\n",
      "log-loss: 2.268 speed: 2230 wps\n",
      "Epoch 8, train log-loss: 2.260, train eff: 0.42290996930565394\n",
      "Epoch 8, valid log-loss: 6.764, valid eff: 0.16704423512578992\n",
      "log-loss: 2.008 speed: 2112 wps\n",
      "log-loss: 1.984 speed: 2229 wps\n",
      "log-loss: 1.892 speed: 1978 wps\n",
      "log-loss: 1.908 speed: 2067 wps\n",
      "log-loss: 1.916 speed: 2122 wps\n",
      "log-loss: 1.921 speed: 2091 wps\n",
      "log-loss: 1.928 speed: 2116 wps\n",
      "log-loss: 1.954 speed: 2155 wps\n",
      "log-loss: 1.946 speed: 2168 wps\n",
      "Epoch 9, train log-loss: 1.957, train eff: 0.46280353802284824\n",
      "Epoch 9, valid log-loss: 6.853, valid eff: 0.1713365923452963\n",
      "log-loss: 1.669 speed: 2085 wps\n",
      "log-loss: 1.710 speed: 2243 wps\n",
      "log-loss: 1.737 speed: 2296 wps\n",
      "log-loss: 1.730 speed: 2248 wps\n",
      "log-loss: 1.763 speed: 2320 wps\n",
      "log-loss: 1.731 speed: 2201 wps\n",
      "log-loss: 1.714 speed: 2184 wps\n",
      "log-loss: 1.709 speed: 2183 wps\n",
      "log-loss: 1.714 speed: 2185 wps\n",
      "Epoch 10, train log-loss: 1.712, train eff: 0.5052761583029114\n",
      "Epoch 10, valid log-loss: 6.986, valid eff: 0.16889233337307738\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEXCAYAAABCjVgAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x232578c7eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CollaborativeRecc(object):\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_tags, is_training,\n",
    "            chunk_size=128, batch_size=1, hidden_size=128,\n",
    "            learning_rate=0.1, rho=0.9):\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # placeholders for input data\n",
    "        self._inputs = tf.placeholder(tf.int32, name=\"inputs\",\n",
    "                shape=[batch_size, chunk_size, 3])\n",
    "        self._targets = tf.placeholder(tf.int32, name=\"targets\",\n",
    "                shape=[batch_size, chunk_size])\n",
    "        self._seq_length = tf.placeholder(tf.int32, name=\"seq_length\",\n",
    "                shape=[batch_size])\n",
    "\n",
    "        # RNN cell.\n",
    "        cell = CGRUCell(hidden_size, num_users, num_items, num_tags)\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_\n",
    "                in tf.split(axis=1, num_or_size_splits=chunk_size, value=self._inputs)]\n",
    "        #[O]states, _ = tf.nn.rnn(cell, inputs,\n",
    "        #        initial_state=self._initial_state)\n",
    "\n",
    "        states, _ = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                initial_state=self._initial_state)      #[N]\n",
    "\n",
    "        # Compute the final state for each element of the batch.\n",
    "        self._final_state = tf.gather_nd([self._initial_state] + states,\n",
    "                tf.transpose(tf.stack([self._seq_length, tf.range(batch_size)])))\n",
    "\n",
    "        # Output layer.\n",
    "        # `output` has shape (batch_size * chunk_size, hidden_size).\n",
    "        output = tf.reshape(tf.concat(axis=1, values=states), [-1, hidden_size])\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            ws = tf.get_variable(\"weights\", [hidden_size, num_items + 1],\n",
    "                                 dtype=tf.float32)\n",
    "        # `logits` has shape (batch_size * chunk_size, num_items).\n",
    "        logits = tf.matmul(output, ws)\n",
    "        targets = tf.reshape(self._targets, [-1])\n",
    "        \n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        self._predicts = tf.argmax(\n",
    "            tf.reshape(predictions, [batch_size,chunk_size,num_items+1]), 2)\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "\n",
    "        masked = loss * tf.to_float(tf.sign(targets))\n",
    "        masked = tf.reshape(masked, [batch_size, chunk_size])\n",
    "        self._cost = tf.reduce_sum(masked, axis=1)\n",
    "\n",
    "        if not is_training:\n",
    "            self._train_op = tf.no_op()\n",
    "            return\n",
    "\n",
    "        scalar_cost = tf.reduce_mean(masked)\n",
    "\n",
    "        # Optimization procedure.\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate, decay=rho, epsilon=1e-8)\n",
    "        self._train_op = optimizer.minimize(scalar_cost)\n",
    "        \n",
    "        self._rms_reset = list()\n",
    "        for var in tf.trainable_variables():\n",
    "            slot = optimizer.get_slot(var, \"rms\")\n",
    "            op = slot.assign(tf.zeros(slot.get_shape()))\n",
    "            self._rms_reset.append(op)\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def seq_length(self):\n",
    "        return self._seq_length\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._predicts\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def rms_reset(self):\n",
    "        return self._rms_reset\n",
    "\n",
    "\n",
    "def run_batch(session, model, iterator, initial_state):\n",
    "    \"\"\"Runs the model on all chunks of one batch.\"\"\"\n",
    "    costs = np.zeros(model.batch_size)\n",
    "    sizes = np.zeros(model.batch_size)\n",
    "    correct = 0\n",
    "    state = initial_state\n",
    "    for inputs, targets, seq_len in iterator:\n",
    "        fetches = [model.cost, model.outputs, model.final_state, model.train_op]\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.inputs] = inputs\n",
    "        feed_dict[model.targets] = targets\n",
    "        feed_dict[model.seq_length] = seq_len\n",
    "        feed_dict[model.initial_state] = state\n",
    "        cost, outputs, state, _ = session.run(fetches, feed_dict)\n",
    "        costs += cost\n",
    "        sizes += seq_len\n",
    "        correct += np.sum(outputs == targets)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        errors = costs / sizes\n",
    "    return (errors, np.sum(sizes), state, correct)\n",
    "\n",
    "\n",
    "def run_epoch(session, train_model, valid_model, train_iter, valid_iter,\n",
    "        tot_size):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_errors = list()\n",
    "    valid_errors = list()\n",
    "    tot = 0\n",
    "    train_correct = 0\n",
    "    train_seq_len = 0\n",
    "    valid_correct = 0\n",
    "    valid_seq_len = 0\n",
    "\n",
    "    next_tenth = tot_size / 10\n",
    "\n",
    "    for train, valid in zip(train_iter, valid_iter):\n",
    "        state = session.run(train_model.initial_state)\n",
    "        # Training data.\n",
    "        errors, num_triplets, state, correct = run_batch(\n",
    "                session, train_model, train, state)\n",
    "        tot += num_triplets\n",
    "        train_seq_len += num_triplets\n",
    "        train_correct += correct\n",
    "        train_errors.extend(errors)\n",
    "        # Validation data.\n",
    "        errors, num_triplets, state, correct = run_batch(\n",
    "                session, valid_model, valid, state)\n",
    "        tot += num_triplets\n",
    "        valid_seq_len += num_triplets\n",
    "        valid_correct += correct\n",
    "        valid_errors.extend(errors)\n",
    "\n",
    "        if tot > next_tenth:\n",
    "            print(\"log-loss: {:.3f} speed: {:.0f} wps\".format(\n",
    "                    np.nanmean(train_errors),\n",
    "                    tot / (time.time() - start_time)))\n",
    "            next_tenth += tot_size / 10\n",
    "            \n",
    "        train_eff = train_correct/train_seq_len\n",
    "        valid_eff = valid_correct/valid_seq_len\n",
    "\n",
    "    return (np.nanmean(train_errors), np.nanmean(valid_errors),\n",
    "           train_eff, valid_eff)\n",
    "\n",
    "def main(train_path,valid_path,bs=5,cs=64,hs=128,lr=0.01,\n",
    "         epochs=10,rho=0.9):\n",
    "\n",
    "    train_data = Dataset.from_path(train_path)\n",
    "    valid_data = Dataset.from_path(valid_path)\n",
    "    \n",
    "    num_users = train_data.num_users\n",
    "    num_items = train_data.num_items\n",
    "    num_tags = train_data.num_tags\n",
    "    tot_size = train_data.num_triplets + valid_data.num_triplets\n",
    "\n",
    "    train_data.prepare_batches(cs, bs)\n",
    "    valid_data.prepare_batches(cs, bs, batches_like=train_data)\n",
    "\n",
    "    settings = {\n",
    "        \"chunk_size\": cs,\n",
    "        \"batch_size\": bs,\n",
    "        \"hidden_size\": hs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"rho\": rho,\n",
    "    }\n",
    "    \n",
    "    train_loss = list()\n",
    "    valid_loss = list()\n",
    "    _train_eff = list()\n",
    "    _valid_eff = list()\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        \n",
    "        initializer = tf.random_normal_initializer(\n",
    "                mean=0, stddev=1/sqrt(hs))\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            train_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=True, **settings)\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            valid_model = CollaborativeRecc(num_users, num_items, num_tags,\n",
    "                    is_training=False, **settings)\n",
    "            \n",
    "        tf.global_variables_initializer().run()\n",
    "        session.run(train_model.rms_reset)\n",
    "        for i in range(1, epochs + 1):\n",
    "            order = np.random.permutation(train_data.num_batches)\n",
    "            train_iter = train_data.iter_batches(order=order)\n",
    "            valid_iter = valid_data.iter_batches(order=order)\n",
    "\n",
    "            train_err, valid_err, train_eff, valid_eff = run_epoch(session, train_model, valid_model,\n",
    "                    train_iter, valid_iter, tot_size)\n",
    "            train_loss.append(train_err)\n",
    "            valid_loss.append(valid_err)\n",
    "            _train_eff.append(train_eff)\n",
    "            _valid_eff.append(valid_eff)\n",
    "            \n",
    "            print(\"Epoch {}, train log-loss: {:.3f}, train eff: {}\".format(i, train_err, train_eff))\n",
    "            print(\"Epoch {}, valid log-loss: {:.3f}, valid eff: {}\".format(i, valid_err, valid_eff))\n",
    "     \n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2)\n",
    "\n",
    "    ax0.plot(np.arange(1,epochs+1), train_loss, 'o-')\n",
    "    ax0.plot(np.arange(1,epochs+1), valid_loss, '.-')\n",
    "    ax0.set(xlabel='Epochs', ylabel='Mean Loss (cross-entropy)',\n",
    "       title='Train and Valid loss per Epoch')\n",
    "    \n",
    "    ax1.plot(np.arange(1,epochs+1), _train_eff, 'o-')\n",
    "    ax1.plot(np.arange(1,epochs+1), _valid_eff, '.-')\n",
    "    ax1.set(xlabel='Epochs', ylabel='Efficiency (/100)',\n",
    "       title='Efficiency(/100) per Epoch')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "args = {\n",
    "    \"train_path\" : train_file,\n",
    "    \"valid_path\" : valid_file,\n",
    "    \"bs\" : 5,  # batch size\n",
    "    \"cs\" : 64,  # chunk size\n",
    "    \"hs\" : 128,  # hidden size\n",
    "    \"lr\" : 0.01,  # learning rate\n",
    "    \"epochs\" : 10,\n",
    "    \"rho\" : 0.95,  # RMSProp decay coefficient\n",
    "}\n",
    "\n",
    "main(**args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
